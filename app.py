# app.py (C√≥digo RAG Est√°vel Final)

# 1. INSTALA√á√ïES E CONFIGURA√á√ïES

import os
import streamlit as st
from supabase.client import Client, create_client
from langchain_google_genai import GoogleGenerativeAIEmbeddings 
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv

# Carrega vari√°veis de ambiente (se estiver usando .env localmente)
load_dotenv()

# --- 2. VARI√ÅVEIS DE AMBIENTE E SECRETS (√öltima Vers√£o Robusta com Colchetes) ---
import os
import streamlit as st
# ... (outras importa√ß√µes)

# ATEN√á√ÉO: Os nomes das chaves NO C√ìDIGO devem ser MIN√öSCULOS para evitar conflitos de case.

# Lendo as chaves com a nota√ß√£o de colchetes
GEMINI_API_KEY_SECRET = st.secrets.get("gemini_api_key") 
SUPABASE_URL = st.secrets.get("supabase_url") 
SUPABASE_KEY = st.secrets.get("supabase_key") 

# Se estiver rodando localmente, o valor pode vir de os.environ
if not GEMINI_API_KEY_SECRET:
    GEMINI_API_KEY_SECRET = os.environ.get("GEMINI_API_KEY") 

if not SUPABASE_URL:
    SUPABASE_URL = os.environ.get("SUPABASE_URL") 
    
if not SUPABASE_KEY:
    SUPABASE_KEY = os.environ.get("SUPABASE_KEY") 


# Definindo a vari√°vel de ambiente (necess√°ria para a LangChain)
os.environ['GOOGLE_API_KEY'] = GEMINI_API_KEY_SECRET 
TABLE_NAME = "champlim"

# Remova completamente o bloco 'try/except' anterior!

# Remova os argumentos 'os.environ.get(...)' em todas as linhas, 
# se elas estavam duplicando a tentativa de leitura.

# ... o restante do c√≥digo da fun√ß√£o initialize_clients() permanece o mesmo.

# Inicializar clientes (usando st.cache_resource para n√£o recriar a cada intera√ß√£o)
# Isso √© crucial para o desempenho do Streamlit
@st.cache_resource
def initialize_clients():
    supabase_client = create_client(SUPABASE_URL, SUPABASE_KEY)
    
    # NOVO EMBEDDER: Usa a API do Google (muito mais est√°vel no Streamlit)
    embedder = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    
    model = ChatGoogleGenerativeAI(model="gemini-2.5-flash")
    return supabase_client, embedder, model

supabase, embedder, model = initialize_clients()

# -------------------------------------------------------------
# 3. FUN√á√ÉO DE BUSCA RAG (k=40) - L√≥gica de Backend
# -------------------------------------------------------------
def ask_rag(query):
    query_vector = embedder.embed_query(query)

    rpc_params = {
        'query_embedding': query_vector,
        'match_count': 40  # K=40 para evitar timeout no free tier (problema resolvido)
    }

    # Chama RPC (fun√ß√£o vector_search)
    response = supabase.rpc('vector_search', rpc_params).execute()

    context = ""
    if response.data:
        for item in response.data:
            metadata = item.get('metadata', {})
            page = metadata.get('page')
            
            # Fonte CORRIGIDA: Lendo a coluna 'file_name' (que corrigimos no SQL)
            file_name_col = item.get('file_name', 'R. N. Champlin - Enciclop√©dia')
            
            # Formata√ß√£o final da fonte
            source = f" (Fonte: {file_name_col}, P√°gina: {page})" if page is not None else f" (Fonte: {file_name_col})"
            
            context += item.get('content', '') + source + "\n\n---\n\n"
    else:
        # Retorna mensagem se a busca n√£o encontrar nada
        return "Desculpe, n√£o encontrei informa√ß√µes relevantes em meus volumes indexados."


    # Prompt e Chamada ao Gemini
    prompt = ChatPromptTemplate.from_messages([
        ("system", "Voc√™ √© um assistente de estudo b√≠blico. Use o CONTEXTO fornecido para responder √† PERGUNTA. Priorize a informa√ß√£o que corresponde a qualquer refer√™ncia b√≠blica ou tema espec√≠fico mencionado na PERGUNTA. Se a resposta n√£o estiver no contexto, diga 'CONTEXTO N√ÉO ENCONTRADO'. Inclua as fontes (P√°gina e Arquivo) no final de cada resposta."),
        ("user", "CONTEXTO: {context}\n\nPERGUNTA: {question}")
    ])
    
    chain = prompt | model
    result = chain.invoke({"context": context, "question": query})

    return result.content


# -------------------------------------------------------------
# 4. INTERFACE STREAMLIT
# -------------------------------------------------------------
st.set_page_config(page_title="Enciclop√©dia B√≠blica RAG", layout="wide")
st.title("üìö Caf√© com Biblia")
st.markdown("Fa√ßa uma pergunta ou deixe um refer~encia biblica.")

# Coluna para a entrada do usu√°rio
user_query = st.text_input("Sua Pergunta de Estudo B√≠blico:", key="query_input")

if st.button("Buscar Resposta"):
    if user_query:
        # Use um container para o spinner e para a resposta
        with st.spinner("Buscando e analisando o contexto nos 13 volumes..."):
            # Chama a fun√ß√£o principal de RAG
            answer = ask_rag(user_query)
        
        # Exibe a resposta formatada
        st.subheader("Resposta da Enciclop√©dia:")
        st.markdown(answer)
    else:
        st.warning("Por favor, digite uma pergunta.")





