# app.py: C√ìDIGO FINAL E COMPLETO PARA APLICA√á√ÉO WEB (STREAMLIT)

import os
import streamlit as st
from supabase import create_client
from langchain_community.vectorstores import SupabaseVectorStore 
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

# --- 1. CONFIGURA√á√ÉO DE CHAVES E CONEX√ÉO (LENDO DE secrets do Streamlit de forma SIMPLES) ---
try:
    # Lendo as chaves como vari√°veis de ambiente planas e EXIGINDO a leitura aninhada
    GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]
    
    # A leitura ABAIXO s√≥ funciona se o painel tiver a se√ß√£o [supabase]
    SUPABASE_URL = st.secrets["supabase"]["URL"]
    SUPABASE_KEY = st.secrets["supabase"]["KEY"]
    SUPABASE_TABLE_NAME = st.secrets["supabase"]["TABLE_NAME"]
    
except KeyError as e:
    st.error(f"Erro ao ler chave essencial: {e}. Verifique se o painel de Secrets (Settings -> Secrets) cont√©m a chave exata e se o formato TOML √© estrito (sem espa√ßos ap√≥s o sinal de =).")
    # Defina chaves vazias para evitar que o programa pare de rodar
    GEMINI_API_KEY = ""
    SUPABASE_URL = ""
    SUPABASE_KEY = ""
    SUPABASE_TABLE_NAME = "champlim"

# Cliente Supabase e Modelo de Embeddings
# O decorador @st.cache_resource garante que esta fun√ß√£o seja rodada apenas uma vez.
@st.cache_resource
def setup_vector_store():
    """Inicializa clientes e o Vector Store, rodando apenas uma vez."""
    # Garante que as chaves est√£o preenchidas antes de tentar conectar
    if not SUPABASE_URL or not SUPABASE_KEY:
        return None
        
    try:
        supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
        embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")

        # Inicializa o Vector Store: For√ßamos a busca pela fun√ß√£o 'match_documents'
        vector_store = SupabaseVectorStore(
            client=supabase, 
            table_name=SUPABASE_TABLE_NAME,
            embedding=embeddings,
            query_name='match_documents' 
        )
        return vector_store
    except Exception as e:
        # st.exception(e) # Exibe o erro completo para debug
        st.error(f"Erro ao conectar ao Supabase: Verifique o URL/KEY e o √≠ndice no banco de dados.")
        return None

vector_store = setup_vector_store()

# --- 2. FUN√á√ÉO CENTRAL RAG ---

def ask_rag(question: str, vector_store):
    """Realiza a busca RAG e retorna a resposta do Gemini."""
    
    if not vector_store or not GEMINI_API_KEY:
        return "Erro de conex√£o. Verifique as configura√ß√µes.", []

    # Configura o Modelo de Linguagem (Gemini)
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", 
                                 temperature=0.2, 
                                 api_key=GEMINI_API_KEY)

    # ‚ö†Ô∏è PERSONA DO SISTEMA: Aqui voc√™ pode colocar as instru√ß√µes detalhadas 
    # que voc√™ criou para seu GEM no Google AI Studio.
    system_prompt = (
        "Voc√™ √© um assistente de IA especialista em teologia e hist√≥ria. "
        "Use o contexto fornecido da enciclop√©dia Champlim para responder √† pergunta. "
        "Sua resposta deve ser acad√™mica, clara e concisa. "
        "Se a resposta N√ÉO estiver no contexto fornecido, responda apenas: 'N√£o foi poss√≠vel encontrar informa√ß√£o suficiente na enciclop√©dia para responder a esta pergunta.'"
        "\n\nContexto: {context}"
    )
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{input}")
    ])
    document_chain = create_stuff_documents_chain(llm, prompt)

    # Configura o Retriever (Busca otimizada)
    retriever = vector_store.as_retriever(search_kwargs={"k": 8}) 
    
    rag_chain = create_retrieval_chain(retriever, document_chain)
    
    # Executa a Cadeia
    response = rag_chain.invoke({"input": question})
    
    return response["answer"], response["context"]

# --- 3. INTERFACE STREAMLIT ---

st.set_page_config(page_title="Enciclop√©dia B√≠blica RAG (Gemini + Supabase)", layout="wide")

st.title("üìö Enciclop√©dia B√≠blica: Pergunte ao Gemini")
st.markdown("Acesse todo o conte√∫do da enciclop√©dia para obter respostas detalhadas e verificadas.")

# Campo de entrada de texto
user_input = st.text_area("Digite sua pergunta aqui:", placeholder="Ex: Qual a import√¢ncia da Arca da Alian√ßa e onde ela √© mencionada pela √∫ltima vez?")

if st.button("Buscar Resposta"):
    if user_input:
        if not vector_store:
            st.warning("O sistema n√£o p√¥de se conectar ao banco de dados. Verifique as chaves em 'secrets.toml'.")
        else:
            with st.spinner("Buscando e analisando o contexto na enciclop√©dia..."):
                answer, context_docs = ask_rag(user_input, vector_store)
            
            # Exibir a Resposta
            st.subheader("ü§ñ Resposta do Especialista")
            st.write(answer)
            
            # Exibir o Contexto Utilizado
            with st.expander("Ver Contexto Encontrado (Provas)"):
                if context_docs:
                    for doc in context_docs:
                        st.success(f"**[Fonte: {doc.metadata.get('file_name', SUPABASE_TABLE_NAME)}]**")
                        st.text(doc.page_content[:500] + "...") # Limita o preview do texto
                else:

                    st.warning("Nenhum contexto relevante foi encontrado. Tente reformular a pergunta.")










